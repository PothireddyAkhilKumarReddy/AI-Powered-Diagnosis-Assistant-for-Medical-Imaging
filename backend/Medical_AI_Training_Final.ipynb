{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üè• Medical AI Bot - High Accuracy Training (DenseNet121) üè•\n",
                "\n",
                "This notebook is designed to train a **highly accurate** medical image classifier.\n",
                "We use **DenseNet121**, a powerful architecture for X-ray analysis, and automatically download high-quality data from Kaggle.\n",
                "\n",
                "### üöÄ Step 1: Initialize & Authenticate\n",
                "1.  Upload your **`kaggle.json`** file below (Get it from your [Kaggle Account](https://www.kaggle.com/account) -> API -> Create New Token)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
                    ]
                },
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'google.colab'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install -q tf-keras kaggle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Force TensorFlow to use Keras 2 (legacy) format\u001b[39;00m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_USE_LEGACY_KERAS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
                    ]
                }
            ],
            "source": [
                "!pip install -q tf-keras kaggle\n",
                "import os\n",
                "from google.colab import files\n",
                "\n",
                "# Force TensorFlow to use Keras 2 (legacy) format\n",
                "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
                "\n",
                "# Upload kaggle.json\n",
                "if not os.path.exists('kaggle.json'):\n",
                "    print(\"Upload your kaggle.json file:\")\n",
                "    files.upload()\n",
                "\n",
                "# Configure Kaggle\n",
                "!mkdir -p ~/.kaggle\n",
                "!cp kaggle.json ~/.kaggle/\n",
                "!chmod 600 ~/.kaggle/kaggle.json\n",
                "print(\"‚úÖ Kaggle Configured Successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üì• Step 2: Download & Prepare Data\n",
                "We will download TWO datasets to ensure we have enough diversity:\n",
                "1.  **COVID-19 Radiography Database** (Good for COVID/Normal)\n",
                "2.  **Chest X-Ray Prediction** (Good for Pneumonia)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"‚è≥ Downloading Datasets... Please wait.\")\n",
                "\n",
                "# 1. Download COVID-19 Radiography Database\n",
                "if not os.path.exists('covid19-radiography-database.zip'):\n",
                "    !kaggle datasets download -d tawsifurrahman/covid19-radiography-database\n",
                "    !unzip -q covid19-radiography-database.zip\n",
                "    print(\"‚úÖ COVID-19 Database Downloaded.\")\n",
                "\n",
                "# 2. Download Pneumonia Dataset\n",
                "if not os.path.exists('chest-xray-pneumonia.zip'):\n",
                "    !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
                "    !unzip -q chest-xray-pneumonia.zip\n",
                "    print(\"‚úÖ Pneumonia Database Downloaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "import random\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Setup Dataset Directory\n",
                "DATASET_DIR = 'dataset'\n",
                "if os.path.exists(DATASET_DIR):\n",
                "    shutil.rmtree(DATASET_DIR)\n",
                "os.makedirs(DATASET_DIR)\n",
                "\n",
                "CLASSES = ['Normal', 'COVID-19', 'Pneumonia']\n",
                "for c in CLASSES:\n",
                "    os.makedirs(os.path.join(DATASET_DIR, c), exist_ok=True)\n",
                "\n",
                "print(\"üìÇ Organizing Data...\")\n",
                "\n",
                "# --- 1. Process COVID-19 Images ---\n",
                "covid_src = os.path.join('COVID-19_Radiography_Dataset', 'COVID', 'images')\n",
                "dst = os.path.join(DATASET_DIR, 'COVID-19')\n",
                "files = [f for f in os.listdir(covid_src) if f.lower().endswith('.png')]\n",
                "# Use ALL COVID images (usually ~3600)\n",
                "for f in tqdm(files, desc=\"Copying COVID\"):\n",
                "    shutil.copy(os.path.join(covid_src, f), os.path.join(dst, f))\n",
                "\n",
                "# --- 2. Process Normal Images ---\n",
                "# We limit Normal images to match COVID count roughly to avoid imbalance\n",
                "normal_src = os.path.join('COVID-19_Radiography_Dataset', 'Normal', 'images')\n",
                "dst = os.path.join(DATASET_DIR, 'Normal')\n",
                "files = [f for f in os.listdir(normal_src) if f.lower().endswith('.png')]\n",
                "selected_files = random.sample(files, min(len(files), 4000)) # improved balance\n",
                "for f in tqdm(selected_files, desc=\"Copying Normal\"):\n",
                "    shutil.copy(os.path.join(normal_src, f), os.path.join(dst, f))\n",
                "\n",
                "# --- 3. Process Pneumonia Images ---\n",
                "pneum_src = os.path.join('chest_xray', 'train', 'PNEUMONIA')\n",
                "dst = os.path.join(DATASET_DIR, 'Pneumonia')\n",
                "files = [f for f in os.listdir(pneum_src) if f.lower().endswith('.jpeg')]\n",
                "# Pneumonia dataset is large (~3800), take 4000 to match\n",
                "selected_files = files[:4000] \n",
                "for f in tqdm(selected_files, desc=\"Copying Pneumonia\"):\n",
                "    shutil.copy(os.path.join(pneum_src, f), os.path.join(dst, f))\n",
                "\n",
                "print(\"\\n‚úÖ Data Preparation Complete!\")\n",
                "for c in CLASSES:\n",
                "    print(f\"   {c}: {len(os.listdir(os.path.join(DATASET_DIR, c)))} images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üß† Step 3: Build & Train Model (DenseNet121)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import layers, models, optimizers\n",
                "from tensorflow.keras.applications import DenseNet121\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "import numpy as np\n",
                "\n",
                "# Configuration\n",
                "IMG_SIZE = (224, 224)\n",
                "BATCH_SIZE = 64 # Increased batch size for T4 GPU utilization\n",
                "EPOCHS = 30\n",
                "DATASET_DIR = \"dataset\"\n",
                "CLASSES = ['COVID-19', 'Normal', 'Pneumonia']\n",
                "\n",
                "print(\"Loading dataset directly into RAM using high-performance tf.data pipeline...\")\n",
                "AUTOTUNE = tf.data.AUTOTUNE\n",
                "\n",
                "def get_label(file_path):\n",
                "    parts = tf.strings.split(file_path, os.path.sep)\n",
                "    return tf.argmax(parts[-2] == CLASSES)\n",
                "\n",
                "def decode_img(img):\n",
                "    img = tf.io.decode_jpeg(img, channels=3)\n",
                "    img = tf.image.resize(img, IMG_SIZE)\n",
                "    # RETURN AS UINT8 (0-255) TO SAVE RAM DURING CACHING (2GB vs 9GB)\n",
                "    return tf.cast(img, tf.uint8)\n",
                "\n",
                "def process_path(file_path):\n",
                "    label = get_label(file_path)\n",
                "    label = tf.one_hot(label, len(CLASSES))\n",
                "    img = tf.io.read_file(file_path)\n",
                "    img = decode_img(img)\n",
                "    return img, label\n",
                "\n",
                "# Build dataset\n",
                "list_ds = tf.data.Dataset.list_files(str(DATASET_DIR + '/*/*'), shuffle=False)\n",
                "list_ds = list_ds.shuffle(15000, seed=42)\n",
                "image_count = len(list_ds)\n",
                "\n",
                "val_size = int(image_count * 0.2)\n",
                "train_ds = list_ds.skip(val_size)\n",
                "val_ds = list_ds.take(val_size)\n",
                "\n",
                "def augment_and_scale(image, label):\n",
                "    # CONVERT TO FLOAT32 [0, 1] AFTER CACHING\n",
                "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
                "    image = tf.image.random_flip_left_right(image)\n",
                "    image = tf.image.random_brightness(image, 0.2)\n",
                "    return image, label\n",
                "\n",
                "def scale_only(image, label):\n",
                "    # CONVERT TO FLOAT32 [0, 1] AFTER CACHING\n",
                "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
                "    return image, label\n",
                "\n",
                "train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
                "train_ds = train_ds.cache() # CACHE IN RAM (Now fits easily as uint8)\n",
                "train_ds = train_ds.shuffle(buffer_size=2000) # Shuffle every epoch\n",
                "train_ds = train_ds.map(augment_and_scale, num_parallel_calls=AUTOTUNE)\n",
                "train_ds = train_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
                "\n",
                "val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
                "val_ds = val_ds.cache()\n",
                "val_ds = val_ds.map(scale_only, num_parallel_calls=AUTOTUNE)\n",
                "val_ds = val_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
                "\n",
                "def build_model():\n",
                "    base_model = DenseNet121(\n",
                "        weights='imagenet',\n",
                "        include_top=False,\n",
                "        input_shape=IMG_SIZE + (3,)\n",
                "    )\n",
                "    \n",
                "    # Unfreeze the last block for fine-tuning\n",
                "    base_model.trainable = True\n",
                "    for layer in base_model.layers[:-40]:\n",
                "        layer.trainable = False\n",
                "        \n",
                "    inputs = tf.keras.Input(shape=IMG_SIZE + (3,))\n",
                "    x = base_model(inputs)\n",
                "    x = layers.GlobalAveragePooling2D()(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Dropout(0.5)(x)\n",
                "    x = layers.Dense(512, activation='relu')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Dropout(0.3)(x)\n",
                "    outputs = layers.Dense(3, activation='softmax')(x)\n",
                "    \n",
                "    model = tf.keras.Model(inputs, outputs)\n",
                "    \n",
                "    model.compile(\n",
                "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    return model\n",
                "\n",
                "model = build_model()\n",
                "model.summary()\n",
                "\n",
                "# Callbacks\n",
                "checkpoint = ModelCheckpoint(\n",
                "    'best_model.h5',\n",
                "    monitor='val_accuracy',\n",
                "    save_best_only=True,\n",
                "    mode='max',\n",
                "    verbose=1\n",
                ")\n",
                "early_stop = EarlyStopping(monitor='val_accuracy', patience=8, verbose=1, restore_best_weights=True)\n",
                "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
                "\n",
                "print(\"üöÄ Starting High-Speed Training...\")\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=EPOCHS,\n",
                "    validation_data=val_ds,\n",
                "    callbacks=[checkpoint, early_stop, reduce_lr]\n",
                ")\n",
                "\n",
                "# Save Final Model\n",
                "model.save('model.h5')\n",
                "print(\"‚úÖ Model Saved as 'model.h5'\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üìä Step 4: Evaluate & Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "files.download('model.h5')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (windenv)",
            "language": "python",
            "name": "windenv"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
