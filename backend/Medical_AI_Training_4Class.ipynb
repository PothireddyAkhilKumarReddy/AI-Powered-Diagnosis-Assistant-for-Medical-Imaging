{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83c\udfe5 Medical AI Bot - 4-Class Training (DenseNet121) \ud83c\udfe5\n",
                "\n",
                "This notebook trains a **DenseNet121** chest X-ray classifier with **4 classes**:\n",
                "- \u2705 COVID-19\n",
                "- \u2705 Normal\n",
                "- \u2705 Pneumonia\n",
                "- \u2705 **Tuberculosis (TB)** \u2190 NEW!\n",
                "\n",
                "### \ud83d\ude80 Step 1: Initialize & Authenticate\n",
                "1.  Upload your **`kaggle.json`** file below (Get it from your [Kaggle Account](https://www.kaggle.com/account) -> API -> Create New Token)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tf-keras kaggle\n",
                "import os\n",
                "from google.colab import files\n",
                "\n",
                "# Force TensorFlow to use Keras 2 (legacy) format\n",
                "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
                "\n",
                "# Upload kaggle.json\n",
                "if not os.path.exists('kaggle.json'):\n",
                "    print(\"Upload your kaggle.json file:\")\n",
                "    files.upload()\n",
                "\n",
                "# Configure Kaggle\n",
                "!mkdir -p ~/.kaggle\n",
                "!cp kaggle.json ~/.kaggle/\n",
                "!chmod 600 ~/.kaggle/kaggle.json\n",
                "print(\"\u2705 Kaggle Configured Successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udce5 Step 2: Download & Prepare Data (4 Classes)\n",
                "We download THREE datasets:\n",
                "1.  **COVID-19 Radiography Database** (COVID-19 + Normal images)\n",
                "2.  **Chest X-Ray Pneumonia** (Pneumonia images)\n",
                "3.  **Tuberculosis (TB) Chest X-ray Database** (TB images)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\u23f3 Downloading Datasets... Please wait.\")\n",
                "\n",
                "# 1. Download COVID-19 Radiography Database\n",
                "if not os.path.exists('covid19-radiography-database.zip'):\n",
                "    !kaggle datasets download -d tawsifurrahman/covid19-radiography-database\n",
                "    !unzip -q covid19-radiography-database.zip\n",
                "    print(\"\u2705 COVID-19 Database Downloaded.\")\n",
                "\n",
                "# 2. Download Pneumonia Dataset\n",
                "if not os.path.exists('chest-xray-pneumonia.zip'):\n",
                "    !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
                "    !unzip -q chest-xray-pneumonia.zip\n",
                "    print(\"\u2705 Pneumonia Database Downloaded.\")\n",
                "\n",
                "# 3. Download Tuberculosis Dataset (NEW)\n",
                "if not os.path.exists('tuberculosis-tb-chest-xray-dataset.zip'):\n",
                "    !kaggle datasets download -d tawsifurrahman/tuberculosis-tb-chest-xray-dataset\n",
                "    !unzip -q tuberculosis-tb-chest-xray-dataset.zip\n",
                "    print(\"\u2705 Tuberculosis Database Downloaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "import random\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Setup Dataset Directory\n",
                "DATASET_DIR = 'dataset'\n",
                "if os.path.exists(DATASET_DIR):\n",
                "    shutil.rmtree(DATASET_DIR)\n",
                "os.makedirs(DATASET_DIR)\n",
                "\n",
                "CLASSES = ['COVID-19', 'Normal', 'Pneumonia', 'Tuberculosis']\n",
                "for c in CLASSES:\n",
                "    os.makedirs(os.path.join(DATASET_DIR, c), exist_ok=True)\n",
                "\n",
                "print(\"\ud83d\udcc2 Organizing Data (4 Classes)...\")\n",
                "\n",
                "# --- 1. Process COVID-19 Images ---\n",
                "covid_src = os.path.join('COVID-19_Radiography_Dataset', 'COVID', 'images')\n",
                "dst = os.path.join(DATASET_DIR, 'COVID-19')\n",
                "files_list = [f for f in os.listdir(covid_src) if f.lower().endswith('.png')]\n",
                "for f in tqdm(files_list, desc=\"Copying COVID\"):\n",
                "    shutil.copy(os.path.join(covid_src, f), os.path.join(dst, f))\n",
                "\n",
                "# --- 2. Process Normal Images ---\n",
                "normal_src = os.path.join('COVID-19_Radiography_Dataset', 'Normal', 'images')\n",
                "dst = os.path.join(DATASET_DIR, 'Normal')\n",
                "files_list = [f for f in os.listdir(normal_src) if f.lower().endswith('.png')]\n",
                "selected_files = random.sample(files_list, min(len(files_list), 4000))\n",
                "for f in tqdm(selected_files, desc=\"Copying Normal\"):\n",
                "    shutil.copy(os.path.join(normal_src, f), os.path.join(dst, f))\n",
                "\n",
                "# --- 3. Process Pneumonia Images ---\n",
                "pneum_src = os.path.join('chest_xray', 'train', 'PNEUMONIA')\n",
                "dst = os.path.join(DATASET_DIR, 'Pneumonia')\n",
                "files_list = [f for f in os.listdir(pneum_src) if f.lower().endswith('.jpeg')]\n",
                "selected_files = files_list[:4000]\n",
                "for f in tqdm(selected_files, desc=\"Copying Pneumonia\"):\n",
                "    shutil.copy(os.path.join(pneum_src, f), os.path.join(dst, f))\n",
                "\n",
                "# --- 4. Process Tuberculosis Images (NEW) ---\n",
                "tb_src = os.path.join('TB_Chest_Radiography_Database', 'Tuberculosis')\n",
                "dst = os.path.join(DATASET_DIR, 'Tuberculosis')\n",
                "files_list = [f for f in os.listdir(tb_src) if f.lower().endswith('.png')]\n",
                "for f in tqdm(files_list, desc=\"Copying Tuberculosis\"):\n",
                "    shutil.copy(os.path.join(tb_src, f), os.path.join(dst, f))\n",
                "\n",
                "print(\"\\n\u2705 Data Preparation Complete!\")\n",
                "for c in CLASSES:\n",
                "    print(f\"   {c}: {len(os.listdir(os.path.join(DATASET_DIR, c)))} images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83e\udde0 Step 3: Build & Train Model (DenseNet121 - 4 Classes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import layers, models, optimizers\n",
                "from tensorflow.keras.applications import DenseNet121\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "import numpy as np\n",
                "\n",
                "# Configuration\n",
                "IMG_SIZE = (224, 224)\n",
                "BATCH_SIZE = 64\n",
                "EPOCHS = 30\n",
                "DATASET_DIR = \"dataset\"\n",
                "CLASSES = ['COVID-19', 'Normal', 'Pneumonia', 'Tuberculosis']\n",
                "NUM_CLASSES = len(CLASSES)  # 4\n",
                "\n",
                "print(f\"Training {NUM_CLASSES}-class model: {CLASSES}\")\n",
                "AUTOTUNE = tf.data.AUTOTUNE\n",
                "\n",
                "def get_label(file_path):\n",
                "    parts = tf.strings.split(file_path, os.path.sep)\n",
                "    return tf.argmax(parts[-2] == CLASSES)\n",
                "\n",
                "def decode_img(img):\n",
                "    img = tf.io.decode_jpeg(img, channels=3)\n",
                "    img = tf.image.resize(img, IMG_SIZE)\n",
                "    return tf.cast(img, tf.uint8)\n",
                "\n",
                "def process_path(file_path):\n",
                "    label = get_label(file_path)\n",
                "    label = tf.one_hot(label, NUM_CLASSES)\n",
                "    img = tf.io.read_file(file_path)\n",
                "    img = decode_img(img)\n",
                "    return img, label\n",
                "\n",
                "# Build dataset\n",
                "list_ds = tf.data.Dataset.list_files(str(DATASET_DIR + '/*/*'), shuffle=False)\n",
                "list_ds = list_ds.shuffle(15000, seed=42)\n",
                "image_count = len(list_ds)\n",
                "\n",
                "val_size = int(image_count * 0.2)\n",
                "train_ds = list_ds.skip(val_size)\n",
                "val_ds = list_ds.take(val_size)\n",
                "\n",
                "def augment_and_scale(image, label):\n",
                "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
                "    image = tf.image.random_flip_left_right(image)\n",
                "    image = tf.image.random_brightness(image, 0.2)\n",
                "    return image, label\n",
                "\n",
                "def scale_only(image, label):\n",
                "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
                "    return image, label\n",
                "\n",
                "train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
                "train_ds = train_ds.cache()\n",
                "train_ds = train_ds.shuffle(buffer_size=2000)\n",
                "train_ds = train_ds.map(augment_and_scale, num_parallel_calls=AUTOTUNE)\n",
                "train_ds = train_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
                "\n",
                "val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
                "val_ds = val_ds.cache()\n",
                "val_ds = val_ds.map(scale_only, num_parallel_calls=AUTOTUNE)\n",
                "val_ds = val_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
                "\n",
                "def build_model():\n",
                "    base_model = DenseNet121(\n",
                "        weights='imagenet',\n",
                "        include_top=False,\n",
                "        input_shape=IMG_SIZE + (3,)\n",
                "    )\n",
                "    \n",
                "    base_model.trainable = True\n",
                "    for layer in base_model.layers[:-40]:\n",
                "        layer.trainable = False\n",
                "        \n",
                "    inputs = tf.keras.Input(shape=IMG_SIZE + (3,))\n",
                "    x = base_model(inputs)\n",
                "    x = layers.GlobalAveragePooling2D()(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Dropout(0.5)(x)\n",
                "    x = layers.Dense(512, activation='relu')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Dropout(0.3)(x)\n",
                "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)  # 4 classes!\n",
                "    \n",
                "    model = tf.keras.Model(inputs, outputs)\n",
                "    \n",
                "    model.compile(\n",
                "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    return model\n",
                "\n",
                "model = build_model()\n",
                "model.summary()\n",
                "\n",
                "# Callbacks\n",
                "checkpoint = ModelCheckpoint(\n",
                "    'best_model.h5',\n",
                "    monitor='val_accuracy',\n",
                "    save_best_only=True,\n",
                "    mode='max',\n",
                "    verbose=1\n",
                ")\n",
                "early_stop = EarlyStopping(monitor='val_accuracy', patience=8, verbose=1, restore_best_weights=True)\n",
                "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
                "\n",
                "print(\"\ud83d\ude80 Starting 4-Class Training...\")\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=EPOCHS,\n",
                "    validation_data=val_ds,\n",
                "    callbacks=[checkpoint, early_stop, reduce_lr]\n",
                ")\n",
                "\n",
                "# Save Final Model\n",
                "model.save('model.h5')\n",
                "print(\"\u2705 Model Saved as 'model.h5'\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udcca Step 4: Evaluate & Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show training results\n",
                "print(f\"\\n\\n=== FINAL RESULTS ===\")\n",
                "print(f\"Best Val Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
                "print(f\"Final Train Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
                "print(f\"Classes: {CLASSES}\")\n",
                "print(f\"\\nDownloading model.h5...\")\n",
                "\n",
                "from google.colab import files\n",
                "files.download('model.h5')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 4,
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}