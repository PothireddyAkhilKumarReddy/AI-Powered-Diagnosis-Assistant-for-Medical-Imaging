{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83c\udfe5 Medical AI Bot - 6-Class Training (DenseNet121) \ud83c\udfe5\n",
                "\n",
                "This notebook trains a **DenseNet121** chest X-ray classifier with **6 classes**:\n",
                "- \u2705 COVID-19\n",
                "- \u2705 Normal\n",
                "- \u2705 Pneumonia\n",
                "- \u2705 Tuberculosis (TB)\n",
                "- \u2705 **Lung Cancer** \u2190 NEW!\n",
                "- \u2705 **Pleural Effusion** \u2190 NEW!\n",
                "\n",
                "### \ud83d\ude80 Step 1: Initialize & Authenticate\n",
                "1.  Upload your **`kaggle.json`** file below (Get it from your [Kaggle Account](https://www.kaggle.com/account) -> API -> Create New Token)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tf-keras kaggle\n",
                "import os\n",
                "from google.colab import files\n",
                "\n",
                "# Force TensorFlow to use Keras 2 (legacy) format\n",
                "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
                "\n",
                "# Upload kaggle.json\n",
                "if not os.path.exists('kaggle.json'):\n",
                "    print(\"Upload your kaggle.json file:\")\n",
                "    files.upload()\n",
                "\n",
                "# Configure Kaggle\n",
                "!mkdir -p ~/.kaggle\n",
                "!cp kaggle.json ~/.kaggle/\n",
                "!chmod 600 ~/.kaggle/kaggle.json\n",
                "print(\"\u2705 Kaggle Configured Successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udce5 Step 2: Download & Prepare Data (6 Classes)\n",
                "We download FOUR datasets:\n",
                "1.  **COVID-19 Radiography Database** (COVID-19 + Normal images)\n",
                "2.  **Chest X-Ray Pneumonia** (Pneumonia images)\n",
                "3.  **Tuberculosis (TB) Chest X-ray Database** (TB images)\n",
                "4.  **X-ray Lung Diseases (9 classes)** (Lung Cancer + Pleural Effusion images)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\u23f3 Downloading Datasets... Please wait.\")\n",
                "\n",
                "# 1. Download COVID-19 Radiography Database\n",
                "if not os.path.exists('covid19-radiography-database.zip'):\n",
                "    !kaggle datasets download -d tawsifurrahman/covid19-radiography-database\n",
                "    !unzip -q covid19-radiography-database.zip\n",
                "    print(\"\u2705 COVID-19 Database Downloaded.\")\n",
                "\n",
                "# 2. Download Pneumonia Dataset\n",
                "if not os.path.exists('chest-xray-pneumonia.zip'):\n",
                "    !kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
                "    !unzip -q chest-xray-pneumonia.zip\n",
                "    print(\"\u2705 Pneumonia Database Downloaded.\")\n",
                "\n",
                "# 3. Download Tuberculosis Dataset\n",
                "if not os.path.exists('tuberculosis-tb-chest-xray-dataset.zip'):\n",
                "    !kaggle datasets download -d tawsifurrahman/tuberculosis-tb-chest-xray-dataset\n",
                "    !unzip -q tuberculosis-tb-chest-xray-dataset.zip\n",
                "    print(\"\u2705 Tuberculosis Database Downloaded.\")\n",
                "\n",
                "# 4. Download Lung Diseases 9-class Dataset (for Lung Cancer + Pleural Effusion)\n",
                "if not os.path.exists('x-ray-lung-diseases-images-9-classes.zip'):\n",
                "    !kaggle datasets download -d fernando-feltrin/x-ray-lung-diseases-images-9-classes\n",
                "    !unzip -q x-ray-lung-diseases-images-9-classes.zip -d lung_diseases_9class\n",
                "    print(\"\u2705 Lung Diseases 9-Class Database Downloaded.\")\n",
                "\n",
                "# List the 9-class dataset structure\n",
                "print(\"\\n\ud83d\udcc2 9-class dataset folders:\")\n",
                "if os.path.exists('lung_diseases_9class'):\n",
                "    for item in sorted(os.listdir('lung_diseases_9class')):\n",
                "        full = os.path.join('lung_diseases_9class', item)\n",
                "        if os.path.isdir(full):\n",
                "            count = len([f for f in os.listdir(full) if not f.startswith('.')])\n",
                "            print(f'   {item}: {count} images')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "import random\n",
                "from tqdm import tqdm\n",
                "import glob\n",
                "\n",
                "# Setup Dataset Directory\n",
                "DATASET_DIR = 'dataset'\n",
                "if os.path.exists(DATASET_DIR):\n",
                "    shutil.rmtree(DATASET_DIR)\n",
                "os.makedirs(DATASET_DIR)\n",
                "\n",
                "CLASSES = ['COVID-19', 'Lung Cancer', 'Normal', 'Pleural Effusion', 'Pneumonia', 'Tuberculosis']\n",
                "for c in CLASSES:\n",
                "    os.makedirs(os.path.join(DATASET_DIR, c), exist_ok=True)\n",
                "\n",
                "print(\"\ud83d\udcc2 Organizing Data (6 Classes)...\")\n",
                "\n",
                "def copy_images(src_dir, dst_dir, max_count=None, extensions=('.png', '.jpg', '.jpeg')):\n",
                "    \"\"\"Copy images from src to dst, optionally limiting count.\"\"\"\n",
                "    files_list = [f for f in os.listdir(src_dir) if f.lower().endswith(extensions)]\n",
                "    if max_count and len(files_list) > max_count:\n",
                "        files_list = random.sample(files_list, max_count)\n",
                "    for f in tqdm(files_list, desc=f'  -> {os.path.basename(dst_dir)}'):\n",
                "        shutil.copy(os.path.join(src_dir, f), os.path.join(dst_dir, f))\n",
                "    return len(files_list)\n",
                "\n",
                "# --- 1. COVID-19 Images ---\n",
                "covid_src = os.path.join('COVID-19_Radiography_Dataset', 'COVID', 'images')\n",
                "copy_images(covid_src, os.path.join(DATASET_DIR, 'COVID-19'))\n",
                "\n",
                "# --- 2. Normal Images ---\n",
                "normal_src = os.path.join('COVID-19_Radiography_Dataset', 'Normal', 'images')\n",
                "copy_images(normal_src, os.path.join(DATASET_DIR, 'Normal'), max_count=4000)\n",
                "\n",
                "# --- 3. Pneumonia Images ---\n",
                "pneum_src = os.path.join('chest_xray', 'train', 'PNEUMONIA')\n",
                "copy_images(pneum_src, os.path.join(DATASET_DIR, 'Pneumonia'), max_count=4000)\n",
                "\n",
                "# --- 4. Tuberculosis Images ---\n",
                "tb_src = os.path.join('TB_Chest_Radiography_Database', 'Tuberculosis')\n",
                "copy_images(tb_src, os.path.join(DATASET_DIR, 'Tuberculosis'))\n",
                "\n",
                "# --- 5. Lung Cancer Images (from 9-class dataset) ---\n",
                "# Look for folder names containing 'cancer', 'tumor', 'mass', or 'nodule'\n",
                "lung_cancer_candidates = []\n",
                "if os.path.exists('lung_diseases_9class'):\n",
                "    for item in os.listdir('lung_diseases_9class'):\n",
                "        lower = item.lower()\n",
                "        if any(kw in lower for kw in ['cancer', 'tumor', 'mass', 'nodule', 'encapsulated']):\n",
                "            lung_cancer_candidates.append(item)\n",
                "    print(f'\\n  Lung Cancer candidate folders: {lung_cancer_candidates}')\n",
                "    for folder in lung_cancer_candidates:\n",
                "        src = os.path.join('lung_diseases_9class', folder)\n",
                "        if os.path.isdir(src):\n",
                "            copy_images(src, os.path.join(DATASET_DIR, 'Lung Cancer'))\n",
                "\n",
                "# --- 6. Pleural Effusion Images (from 9-class dataset) ---\n",
                "effusion_candidates = []\n",
                "if os.path.exists('lung_diseases_9class'):\n",
                "    for item in os.listdir('lung_diseases_9class'):\n",
                "        lower = item.lower()\n",
                "        if any(kw in lower for kw in ['effusion', 'pleural']):\n",
                "            effusion_candidates.append(item)\n",
                "    print(f'  Pleural Effusion candidate folders: {effusion_candidates}')\n",
                "    for folder in effusion_candidates:\n",
                "        src = os.path.join('lung_diseases_9class', folder)\n",
                "        if os.path.isdir(src):\n",
                "            copy_images(src, os.path.join(DATASET_DIR, 'Pleural Effusion'))\n",
                "\n",
                "print(\"\\n\u2705 Data Preparation Complete!\")\n",
                "for c in CLASSES:\n",
                "    count = len(os.listdir(os.path.join(DATASET_DIR, c)))\n",
                "    print(f\"   {c}: {count} images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83e\udde0 Step 3: Build & Train Model (DenseNet121 - 6 Classes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import layers, models, optimizers\n",
                "from tensorflow.keras.applications import DenseNet121\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "import numpy as np\n",
                "\n",
                "# Configuration\n",
                "IMG_SIZE = (224, 224)\n",
                "BATCH_SIZE = 64\n",
                "EPOCHS = 30\n",
                "DATASET_DIR = \"dataset\"\n",
                "CLASSES = ['COVID-19', 'Lung Cancer', 'Normal', 'Pleural Effusion', 'Pneumonia', 'Tuberculosis']\n",
                "NUM_CLASSES = len(CLASSES)  # 6\n",
                "\n",
                "print(f\"Training {NUM_CLASSES}-class model: {CLASSES}\")\n",
                "AUTOTUNE = tf.data.AUTOTUNE\n",
                "\n",
                "def get_label(file_path):\n",
                "    parts = tf.strings.split(file_path, os.path.sep)\n",
                "    return tf.argmax(parts[-2] == CLASSES)\n",
                "\n",
                "def decode_img(img):\n",
                "    img = tf.io.decode_jpeg(img, channels=3)\n",
                "    img = tf.image.resize(img, IMG_SIZE)\n",
                "    return tf.cast(img, tf.uint8)\n",
                "\n",
                "def process_path(file_path):\n",
                "    label = get_label(file_path)\n",
                "    label = tf.one_hot(label, NUM_CLASSES)\n",
                "    img = tf.io.read_file(file_path)\n",
                "    img = decode_img(img)\n",
                "    return img, label\n",
                "\n",
                "# Build dataset\n",
                "list_ds = tf.data.Dataset.list_files(str(DATASET_DIR + '/*/*'), shuffle=False)\n",
                "list_ds = list_ds.shuffle(20000, seed=42)\n",
                "image_count = len(list_ds)\n",
                "\n",
                "val_size = int(image_count * 0.2)\n",
                "train_ds = list_ds.skip(val_size)\n",
                "val_ds = list_ds.take(val_size)\n",
                "\n",
                "def augment_and_scale(image, label):\n",
                "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
                "    image = tf.image.random_flip_left_right(image)\n",
                "    image = tf.image.random_brightness(image, 0.2)\n",
                "    return image, label\n",
                "\n",
                "def scale_only(image, label):\n",
                "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
                "    return image, label\n",
                "\n",
                "train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
                "train_ds = train_ds.cache()\n",
                "train_ds = train_ds.shuffle(buffer_size=2000)\n",
                "train_ds = train_ds.map(augment_and_scale, num_parallel_calls=AUTOTUNE)\n",
                "train_ds = train_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
                "\n",
                "val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
                "val_ds = val_ds.cache()\n",
                "val_ds = val_ds.map(scale_only, num_parallel_calls=AUTOTUNE)\n",
                "val_ds = val_ds.batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
                "\n",
                "def build_model():\n",
                "    base_model = DenseNet121(\n",
                "        weights='imagenet',\n",
                "        include_top=False,\n",
                "        input_shape=IMG_SIZE + (3,)\n",
                "    )\n",
                "    \n",
                "    base_model.trainable = True\n",
                "    for layer in base_model.layers[:-40]:\n",
                "        layer.trainable = False\n",
                "        \n",
                "    inputs = tf.keras.Input(shape=IMG_SIZE + (3,))\n",
                "    x = base_model(inputs)\n",
                "    x = layers.GlobalAveragePooling2D()(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Dropout(0.5)(x)\n",
                "    x = layers.Dense(512, activation='relu')(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Dropout(0.3)(x)\n",
                "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)  # 6 classes!\n",
                "    \n",
                "    model = tf.keras.Model(inputs, outputs)\n",
                "    \n",
                "    model.compile(\n",
                "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
                "        loss='categorical_crossentropy',\n",
                "        metrics=['accuracy']\n",
                "    )\n",
                "    return model\n",
                "\n",
                "model = build_model()\n",
                "model.summary()\n",
                "\n",
                "# Callbacks\n",
                "checkpoint = ModelCheckpoint(\n",
                "    'best_model.h5',\n",
                "    monitor='val_accuracy',\n",
                "    save_best_only=True,\n",
                "    mode='max',\n",
                "    verbose=1\n",
                ")\n",
                "early_stop = EarlyStopping(monitor='val_accuracy', patience=8, verbose=1, restore_best_weights=True)\n",
                "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
                "\n",
                "print(\"\ud83d\ude80 Starting 6-Class Training...\")\n",
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=EPOCHS,\n",
                "    validation_data=val_ds,\n",
                "    callbacks=[checkpoint, early_stop, reduce_lr]\n",
                ")\n",
                "\n",
                "# Save Final Model\n",
                "model.save('model.h5')\n",
                "print(\"\u2705 Model Saved as 'model.h5'\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### \ud83d\udcca Step 4: Evaluate & Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show training results\n",
                "print(f\"\\n\\n=== FINAL RESULTS ===\")\n",
                "print(f\"Best Val Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
                "print(f\"Final Train Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
                "print(f\"Classes: {CLASSES}\")\n",
                "print(f\"\\nDownloading model.h5...\")\n",
                "\n",
                "from google.colab import files\n",
                "files.download('model.h5')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_minor": 4,
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}